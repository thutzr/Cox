{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pycox\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.datasets import metabric\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import CoxPH,CoxCC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from generate_data import load_data\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from weighted_conformal_prediction_coxph import WeightedConformalPrediction\n",
    "from datetime import datetime\n",
    "from pycox.datasets import metabric,support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\t0.992\t171.000\t207.000\n",
      "[1]\t1.000\t160.000\t221.000\n",
      "[2]\t0.997\t145.000\t235.000\n",
      "[3]\t1.000\t165.000\t216.000\n",
      "[4]\t1.000\t160.000\t221.000\n",
      "[5]\t0.990\t174.000\t203.000\n",
      "[6]\t0.990\t167.000\t210.000\n",
      "[7]\t0.995\t160.000\t219.000\n",
      "[8]\t0.997\t163.000\t217.000\n",
      "[9]\t0.997\t155.000\t225.000\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.95\n",
    "\n",
    "method = 'vanilla_cox_ph'\n",
    "data = 'metabric'\n",
    "\n",
    "df = metabric.read_df()\n",
    "epochs = 10\n",
    "train_frac = 0.6\n",
    "test_frac = 0.2\n",
    "val_frac = 0.2\n",
    "coverage = []\n",
    "coverage_censor = []\n",
    "coverage_non_censor = []\n",
    "for epoch in range(epochs):\n",
    "      rng = np.random.RandomState(epoch)\n",
    "      shuffle_idx = rng.permutation(range(len(df)))\n",
    "      train_idx = shuffle_idx[:int(train_frac*len(df))]\n",
    "      val_idx = shuffle_idx[int(train_frac*len(df)):int((train_frac+val_frac)*len(df))]\n",
    "      test_idx = shuffle_idx[int((train_frac+val_frac)*len(df)):]\n",
    "\n",
    "      df_train = df.iloc[train_idx,:]\n",
    "      df_val = df.iloc[val_idx,:]\n",
    "      df_test = df.iloc[test_idx,:]\n",
    "      # cols_standardize = ['x0', 'x7', 'x8','x9','x10','x11','x12','x13']\n",
    "      # cols_leave = ['x1', 'x2', 'x3', 'x4','x5','x6']\n",
    "      cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "      cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "#       cols_standardize = ['x0', 'x1', 'x2']\n",
    "      # cols_leave = ['x1', 'x2', 'x3', 'x4','x5','x6']\n",
    "      standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "      # polyfeature = [([col], PolynomialFeatures()) for col in cols_standardize]\n",
    "      leave = [(col, None) for col in cols_leave]\n",
    "      x_mapper = DataFrameMapper(standardize+leave)\n",
    "      \n",
    "      x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "      x_val = x_mapper.transform(df_val).astype('float32')\n",
    "      x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "      get_target = lambda df:(df['duration'].values,df['event'].values)\n",
    "      y_train = get_target(df_train)\n",
    "      y_val = get_target(df_val)\n",
    "      duration_test, event_test = get_target(df_test)\n",
    "\n",
    "      val = tt.tuplefy(x_val,y_val)\n",
    "\n",
    "      in_features = x_train.shape[1]\n",
    "      num_nodes = [32,32]\n",
    "      out_features = 1\n",
    "      batch_norm = True\n",
    "      dropout = 0.1\n",
    "      output_bias = False\n",
    "      \n",
    "      net = tt.practical.MLPVanilla(in_features=in_features, num_nodes=num_nodes, out_features = out_features, batch_norm = batch_norm, dropout = dropout, output_bias = output_bias)\n",
    "\n",
    "      model = CoxPH(net,torch.optim.Adam)\n",
    "\n",
    "      batch_size = 256\n",
    "      n_epochs = 512\n",
    "      verbose = False\n",
    "      callbacks = [tt.callbacks.EarlyStopping()]\n",
    "      model.fit(x_train,y_train,batch_size,n_epochs,callbacks,verbose,val_data=val.repeat(10).cat())\n",
    "      _ = model.compute_baseline_hazards()\n",
    "\n",
    "      surv = model.predict_surv_df(x_test)\n",
    "      surv_ = (surv<=1-alpha).to_numpy(dtype='int8')\n",
    "      index = np.array(surv.index)\n",
    "      multiply_surv = np.transpose(surv_)*index\n",
    "      multiply_surv_ = np.where(multiply_surv==0,np.max(index),multiply_surv)\n",
    "\n",
    "      t_predict = multiply_surv_.min(axis = 1)\n",
    "      diff_predict_true = np.subtract(t_predict,np.array(df_test['duration']))\n",
    "\n",
    "      cover = sum(diff_predict_true>=0)/len(t_predict)\n",
    "      censor = 0\n",
    "      non_censor = 0\n",
    "      for i in range(len(df_test)):\n",
    "            if (diff_predict_true[i] >= 0):\n",
    "                if (event_test[i]==0):\n",
    "                    censor += 1\n",
    "                else:\n",
    "                      non_censor += 1\n",
    "      \n",
    "      coverage.append(cover)\n",
    "      n_censor = len(df_test) - sum(df_test['event'])\n",
    "      if n_censor == 0:\n",
    "            coverage_censor.append(alpha)\n",
    "            coverage_non_censor.append(cover)\n",
    "      elif n_censor == len(df_test):\n",
    "            coverage_non_censor.append(alpha)\n",
    "            coverage_censor.append(cover)\n",
    "      else:\n",
    "            coverage_censor.append(censor/n_censor)\n",
    "            coverage_non_censor.append(non_censor/(len(df_test)-n_censor))\n",
    "            \n",
    "      print('[%d]\\t%.3f\\t%.3f\\t%.3f'%(epoch,cover,censor,non_censor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Coverage Statistics:\t [Mean]0.996\t[Std.]0.004\t[Max]1.000\t[Min]0.990\n"
     ]
    }
   ],
   "source": [
    "print('Total Coverage Statistics:\\t [Mean]%.3f\\t[Std.]%.3f\\t[Max]%.3f\\t[Min]%.3f'%(np.mean(coverage),np.std(coverage),np.max(coverage),np.min(coverage)))\n",
    "\n",
    "np.savetxt('/home/zeren/Research/Cox/output/anilla_coxcc_coverage_'+data+str(epochs)+'.txt',np.array(coverage))\n",
    "np.savetxt('/home/zeren/Research/Cox/output/vanilla_coxcc_censor_coverage_'+data+str(epochs)+'.txt',np.array(coverage_censor))\n",
    "np.savetxt('/home/zeren/Research/Cox/output/vanilla_coxcc_non_censor_coverage_'+data+str(epochs)+'.txt',np.array(coverage_non_censor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
