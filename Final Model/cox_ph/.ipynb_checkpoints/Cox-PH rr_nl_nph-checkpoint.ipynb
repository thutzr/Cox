{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla CoxPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pycox\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.datasets import metabric\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import CoxPH\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from generate_data import load_data\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from weighted_conformal_prediction_coxph import WeightedConformalPrediction\n",
    "from datetime import datetime\n",
    "from pycox.datasets import metabric,support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\t0.544\t215.000\t873.000\n",
      "[1]\t0.549\t222.000\t876.000\n",
      "[2]\t0.543\t214.000\t872.000\n",
      "[3]\t0.560\t233.000\t888.000\n",
      "[4]\t0.538\t211.000\t865.000\n",
      "[5]\t0.560\t227.000\t894.000\n",
      "[6]\t0.533\t209.000\t858.000\n",
      "[7]\t0.538\t219.000\t857.000\n",
      "[8]\t0.539\t190.000\t889.000\n",
      "[9]\t0.542\t198.000\t886.000\n",
      "Total Coverage Statistics:\t [Mean]0.545\t[Std.]0.009\t[Max]0.560\t[Min]0.533\n",
      "[0]\t0.638\t266.000\t1011.000\n",
      "[1]\t0.630\t267.000\t993.000\n",
      "[2]\t0.640\t275.000\t1006.000\n",
      "[3]\t0.662\t301.000\t1024.000\n",
      "[4]\t0.637\t260.000\t1015.000\n",
      "[5]\t0.646\t279.000\t1014.000\n",
      "[6]\t0.625\t271.000\t978.000\n",
      "[7]\t0.641\t282.000\t1000.000\n",
      "[8]\t0.640\t254.000\t1026.000\n",
      "[9]\t0.647\t258.000\t1037.000\n",
      "Total Coverage Statistics:\t [Mean]0.641\t[Std.]0.010\t[Max]0.662\t[Min]0.625\n",
      "[0]\t0.727\t327.000\t1127.000\n",
      "[1]\t0.714\t317.000\t1110.000\n",
      "[2]\t0.726\t323.000\t1128.000\n",
      "[3]\t0.745\t357.000\t1133.000\n",
      "[4]\t0.739\t325.000\t1152.000\n",
      "[5]\t0.747\t340.000\t1154.000\n",
      "[6]\t0.717\t341.000\t1093.000\n",
      "[7]\t0.730\t342.000\t1118.000\n",
      "[8]\t0.728\t308.000\t1147.000\n",
      "[9]\t0.729\t308.000\t1149.000\n",
      "Total Coverage Statistics:\t [Mean]0.730\t[Std.]0.010\t[Max]0.747\t[Min]0.714\n",
      "[0]\t0.801\t374.000\t1228.000\n",
      "[1]\t0.792\t374.000\t1211.000\n",
      "[2]\t0.795\t368.000\t1222.000\n",
      "[3]\t0.810\t399.000\t1221.000\n",
      "[4]\t0.810\t373.000\t1248.000\n",
      "[5]\t0.809\t373.000\t1245.000\n",
      "[6]\t0.790\t381.000\t1199.000\n",
      "[7]\t0.793\t379.000\t1207.000\n",
      "[8]\t0.803\t355.000\t1251.000\n",
      "[9]\t0.797\t346.000\t1249.000\n",
      "Total Coverage Statistics:\t [Mean]0.800\t[Std.]0.007\t[Max]0.810\t[Min]0.790\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.6,0.7,0.8,0.9]\n",
    "\n",
    "method = 'vanilla_cox_ph'\n",
    "data = 'rr_n_nph'\n",
    "\n",
    "df = load_data('rr_nl_nph.pkl')\n",
    "epochs = 10\n",
    "train_frac = 0.6\n",
    "test_frac = 0.2\n",
    "val_frac = 0.2\n",
    "for alpha in alphas:\n",
    "    coverage = []\n",
    "    coverage_censor = []\n",
    "    coverage_non_censor = []\n",
    "    for epoch in range(epochs):\n",
    "          rng = np.random.RandomState(epoch)\n",
    "          shuffle_idx = rng.permutation(range(len(df)))\n",
    "          train_idx = shuffle_idx[:int(train_frac*len(df))]\n",
    "          val_idx = shuffle_idx[int(train_frac*len(df)):int((train_frac+val_frac)*len(df))]\n",
    "          test_idx = shuffle_idx[int((train_frac+val_frac)*len(df)):]\n",
    "\n",
    "          df_train = df.iloc[train_idx,:]\n",
    "          df_val = df.iloc[val_idx,:]\n",
    "          df_test = df.iloc[test_idx,:]\n",
    "\n",
    "          cols_standardize = ['x0', 'x1', 'x2']\n",
    "\n",
    "          standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "          polyfeature = [([col], PolynomialFeatures()) for col in cols_standardize]\n",
    "          x_mapper = DataFrameMapper(standardize+polyfeature)\n",
    "\n",
    "          x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "          x_val = x_mapper.transform(df_val).astype('float32')\n",
    "          x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "          get_target = lambda df:(df['duration'].values,df['event'].values)\n",
    "          y_train = get_target(df_train)\n",
    "          y_val = get_target(df_val)\n",
    "          duration_test, event_test = get_target(df_test)\n",
    "\n",
    "          val = tt.tuplefy(x_val,y_val)\n",
    "\n",
    "          in_features = x_train.shape[1]\n",
    "          num_nodes = [32,32]\n",
    "          out_features = 1\n",
    "          batch_norm = True\n",
    "          dropout = 0.1\n",
    "          output_bias = False\n",
    "\n",
    "          net = tt.practical.MLPVanilla(in_features=in_features, num_nodes=num_nodes, out_features = out_features, batch_norm = batch_norm, dropout = dropout, output_bias = output_bias)\n",
    "\n",
    "          model = CoxPH(net,torch.optim.Adam)\n",
    "\n",
    "          batch_size = 256\n",
    "          n_epochs = 512\n",
    "          verbose = False\n",
    "          callbacks = [tt.callbacks.EarlyStopping()]\n",
    "          model.fit(x_train,y_train,batch_size,n_epochs,callbacks,verbose,val_data=val.repeat(10).cat())\n",
    "          _ = model.compute_baseline_hazards()\n",
    "\n",
    "          surv = model.predict_surv_df(x_test)\n",
    "          surv_ = (surv<=1-alpha).to_numpy(dtype='int8')\n",
    "          index = np.array(surv.index)\n",
    "          multiply_surv = np.transpose(surv_)*index\n",
    "          multiply_surv_ = np.where(multiply_surv==0,np.max(index),multiply_surv)\n",
    "\n",
    "          t_predict = multiply_surv_.min(axis = 1)\n",
    "          diff_predict_true = np.subtract(t_predict,np.array(df_test['duration_true']))\n",
    "\n",
    "          cover = sum(diff_predict_true>=0)/len(t_predict)\n",
    "          censor = 0\n",
    "          non_censor = 0\n",
    "          for i in range(len(df_test)):\n",
    "                if (diff_predict_true[i] >= 0):\n",
    "                      if (event_test[i]==0):\n",
    "                            censor += 1\n",
    "                      else:\n",
    "                            non_censor += 1\n",
    "\n",
    "          coverage.append(cover)\n",
    "          n_censor = len(df_test) - sum(df_test['event'])\n",
    "          if n_censor == 0:\n",
    "                coverage_censor.append(alpha)\n",
    "                coverage_non_censor.append(cover)\n",
    "          elif n_censor == len(df_test):\n",
    "                coverage_non_censor.append(alpha)\n",
    "                coverage_censor.append(cover)\n",
    "          else:\n",
    "                coverage_censor.append(censor/n_censor)\n",
    "                coverage_non_censor.append(non_censor/(len(df_test)-n_censor))\n",
    "\n",
    "          print('[%d]\\t%.3f\\t%.3f\\t%.3f'%(epoch,cover,censor,non_censor))\n",
    "\n",
    "    print('Total Coverage Statistics:\\t [Mean]%.3f\\t[Std.]%.3f\\t[Max]%.3f\\t[Min]%.3f'%(np.mean(coverage),np.std(coverage),np.max(coverage),np.min(coverage)))\n",
    "\n",
    "    np.savetxt('./output/vanilla_coxph_coverage_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(coverage))\n",
    "    np.savetxt('./output/vanilla_coxph_censor_coverage_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(coverage_censor))\n",
    "    np.savetxt('./output/vanilla_coxph_non_censor_coverage_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(coverage_non_censor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CocPH+WCCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------|\n",
      "[Method]\tcox_ph\t[Dataset]\tsupport\t[Alpha]\t0.6\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-066f29c9dd49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mwcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeightedConformalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mwcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mwcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_baseline_hazard_non_zero_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# 对所有可能的时间计算对用的at risk的人，以及相应的exp_g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/Cox/Final Model/coxph/weighted_conformal_prediction_coxph.py\u001b[0m in \u001b[0;36mfind_baseline_hazard_non_zero_idx\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_baseline_hazard_non_zero_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network_cox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_hazards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_baseline_hazards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_zero_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_hazards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_hazards\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 计算第一个非零元素的索引\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/Cox/Final Model/coxph/weighted_conformal_prediction_coxph.py\u001b[0m in \u001b[0;36mneural_network_cox\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoxPH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         self.model.fit(self.x_train, self.y_train, self.batch_size, self.epochs, self.callbacks, self.verbose,\n\u001b[0m\u001b[1;32m    100\u001b[0m                 val_data=self.val, val_batch_size=self.batch_size)\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pycox/models/cox.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuplefy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         return super().fit(input, target, batch_size, epochs, callbacks, verbose,\n\u001b[0m\u001b[1;32m     52\u001b[0m                            \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                            **kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtuples/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             val_dataloader = self.make_dataloader(val_data, val_batch_size, shuffle=False,\n\u001b[1;32m    271\u001b[0m                                                   num_workers=num_workers, **kwargs)\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtuples/base.py\u001b[0m in \u001b[0;36mfit_dataloader\u001b[0;34m(self, dataloader, epochs, callbacks, verbose, metrics, val_dataloader)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtuples/optim.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for alpha in alphas:\n",
    "# def Algorithm_1(alpha):\n",
    "      method = 'cox_ph'\n",
    "      data = 'rr_nl_nph'\n",
    "      print('--'*30+'|')\n",
    "      print('[Method]\\t'+method+'\\t[Dataset]\\t'+data+'\\t[Alpha]\\t'+str(alpha))\n",
    "      df = load_data('rr_nl_nph')\n",
    "      df_data = df\n",
    "      epochs = 10\n",
    "      train_frac = 0.8\n",
    "      empirical_coverage = []\n",
    "      empirical_coverage_censor = []\n",
    "      empirical_coverage_non_censor = []\n",
    "      empirical_coverage_2 = []\n",
    "      empirical_coverage_censor_2 = []\n",
    "      empirical_coverage_non_censor_2 = []\n",
    "      interval_length_1 = []\n",
    "      interval_length_2 = []\n",
    "      for epoch in range(epochs):\n",
    "            rng = np.random.RandomState(epoch)\n",
    "            shuffle_idx = rng.permutation(range(len(df_data)))\n",
    "            train_idx = shuffle_idx[:int(train_frac*len(df_data))]\n",
    "            calib_test_idx = shuffle_idx[int(train_frac*len(df_data)):]\n",
    "            df_train = df_data.iloc[train_idx,:]\n",
    "            df_calib_test = df_data.iloc[calib_test_idx,:]\n",
    "            df_calib_test.reset_index(drop = True,inplace = True)\n",
    "            print('--'*30+'|')\n",
    "            print('Initializing Algorithms')\n",
    "            wcp = WeightedConformalPrediction(df_train,verbose = False,percentile=alpha)\n",
    "            wcp.run_preprocessing()\n",
    "            wcp.find_baseline_hazard_non_zero_idx()\n",
    "            # 对所有可能的时间计算对用的at risk的人，以及相应的exp_g\n",
    "            print('--'*30+'|')\n",
    "            print('Pre-compute Hazard At Risk')\n",
    "            max_duration = np.max(df_data['duration'])\n",
    "            min_duration = np.min(df_data['duration'])\n",
    "            step = 0.1\n",
    "            num_steps = int((max_duration-min_duration)/step)+1\n",
    "\n",
    "            try:\n",
    "                  exp_R_list = np.loadtxt('./data/exp_R_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "            except:\n",
    "                  exp_R_list = []\n",
    "                  for i in range(num_steps):\n",
    "                        t = min_duration + i*step\n",
    "                        at_risk = df_train[df_train['duration']>=t]\n",
    "                        if len(at_risk) == 0:\n",
    "                              exp_R_list.append(0)\n",
    "                        else:\n",
    "                              x_risk = wcp.x_mapper.transform(at_risk).astype('float32')\n",
    "                              cumulative_hazards_risk = wcp.model.predict_cumulative_hazards(x_risk)\n",
    "                              exp_R_list.append(np.sum(cumulative_hazards_risk.loc[wcp.non_zero_idx]/wcp.bh))\n",
    "\n",
    "                  np.savetxt('./data/exp_R_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(exp_R_list))\n",
    "\n",
    "            X_test = wcp.x_mapper.transform(df_calib_test)\n",
    "            duration_test,event_test = wcp.get_target(df_calib_test)\n",
    "            \n",
    "            sq_dists = squareform(pdist(X_test,'sqeuclidean'))\n",
    "            kernel_weights = np.exp(-sq_dists)\n",
    "            \n",
    "            random_shuffle =rng.permutation(X_test.shape[0])\n",
    "            n_test_start_idx = X_test.shape[0] // 2\n",
    "            n_calib_2_start_idx = X_test.shape[0] // 4\n",
    "\n",
    "            calibration_indices = random_shuffle[:n_test_start_idx]\n",
    "            calib_2_idx = random_shuffle[n_test_start_idx:n_test_start_idx+n_calib_2_start_idx]\n",
    "            test_idx = random_shuffle[n_test_start_idx+n_calib_2_start_idx:]\n",
    "            # 计算 calibration set 的 non conformal score\n",
    "            print('--'*30+'|')\n",
    "            print('Begin Computing Nonconformal Score of Alg. 1')\n",
    "            df_calib = df_calib_test.iloc[calibration_indices,:]\n",
    "            df_calib_2 = df_calib_test.iloc[calib_2_idx,:]\n",
    "            df_test = df_calib_test.iloc[test_idx,:]\n",
    "\n",
    "            \n",
    "            df_calib_non_censor = df_calib[df_calib['event']==1]\n",
    "            duration_test_non_censor,event_test_non_censor = wcp.get_target(df_calib_non_censor)\n",
    "            calib_non_censor_idx = np.array(df_calib_non_censor.index)\n",
    "            x_ca_non_censor = wcp.x_mapper.transform(df_calib_non_censor).astype('float32')\n",
    "            cumulative_hazards_non_censor = wcp.model.predict_cumulative_hazards(x_ca_non_censor)\n",
    "            exp_g_non_censor = cumulative_hazards_non_censor.loc[wcp.non_zero_idx]/wcp.bh\n",
    "\n",
    "            try:\n",
    "                  nonconformal_score_1 = np.loadtxt('./data/nonconformal_alg_1_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "            except:\n",
    "                  nonconformal_score_1 = []\n",
    "                  for i in range(len(df_calib_non_censor)):\n",
    "                        t = int((duration_test_non_censor[i]-min_duration)/step)\n",
    "                        exp_g_r = exp_R_list[t]\n",
    "                        nonconformal_score_1.append(np.log(exp_g_non_censor[i])-np.log(exp_g_r))\n",
    "                  \n",
    "                  nonconformal_score_1.append(np.inf)\n",
    "                  nonconformal_score_1 = np.array(nonconformal_score_1)\n",
    "                  np.savetxt('./data/nonconformal_alg_1_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',nonconformal_score_1)\n",
    "\n",
    "            sort_indices = np.argsort(nonconformal_score_1)\n",
    "            sorted_nonconformal_score_1 = nonconformal_score_1[sort_indices]\n",
    "            # 随机选取100个x_0\n",
    "            print('--'*30+'|')\n",
    "            print('Begin Testing')\n",
    "            coverage_CI = []\n",
    "            censor_coverage = []\n",
    "            non_censor_coverage = []\n",
    "            interval_len = []\n",
    "\n",
    "            coverage_CI_2 = []\n",
    "            censor_coverage_2 = []\n",
    "            non_censor_coverage_2 = []\n",
    "            interval_len_2 = []\n",
    "            \n",
    "            t1 = datetime.now()\n",
    "            for test_point_idx in test_idx[:100]:\n",
    "                  \n",
    "                  weights = kernel_weights[test_point_idx][calib_non_censor_idx]\n",
    "                  sampling_probs = kernel_weights[test_point_idx][test_idx] # 计算测试集中每个点的抽样概率\n",
    "                  sampling_probs /= sampling_probs.sum()\n",
    "\n",
    "                  # 对第二个calibration set中的点计算nonconformalty score\n",
    "                  try:\n",
    "                        nonconformal_score_2 = np.loadtxt('./data/nonconformal_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "                        calibration_duration = np.loadtxt('./data/calibrated_duration_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "                  except:\n",
    "                        nonconformal_score_2 = []\n",
    "                        calibrated_duration = []\n",
    "                        for calib_point_2 in calib_2_idx:\n",
    "                              sorted_calib_weights = np.append(weights[sort_indices[:-1]],kernel_weights[calib_point_2,test_point_idx])\n",
    "                              sorted_calib_weights /= sorted_calib_weights.sum()\n",
    "                              sorted_calib_weight_cum_sum = np.cumsum(sorted_calib_weights)\n",
    "                              CI_calib_idx = np.min(np.argwhere(sorted_calib_weight_cum_sum>=alpha))\n",
    "                              \n",
    "                              threshold_score_calib = sorted_nonconformal_score_1[int(CI_calib_idx)]\n",
    "                              \n",
    "                              x_test = X_test[calib_point_2].reshape(1,-1)\n",
    "                              cumulative_hazards_calib_2 = wcp.model.predict_cumulative_hazards(x_test)\n",
    "                              exp_g_calib_2 = cumulative_hazards_calib_2.loc[wcp.non_zero_idx]/wcp.bh\n",
    "                              exp_g_r_calib_2 = np.exp(np.log(exp_g_calib_2) - threshold_score_calib)\n",
    "                              candidate_calib_idx = np.argwhere(exp_R_list>=exp_g_r_calib_2[0])\n",
    "\n",
    "                              if len(candidate_calib_idx) == 0:\n",
    "                                    t_calib = max_duration\n",
    "                              else:\n",
    "                                    t_calib = np.max(candidate_calib_idx)*step + min_duration\n",
    "                              nonconformal_score_2.append(max(-duration_test[calib_point_2],duration_test[calib_point_2]-t_calib))\n",
    "\n",
    "                              calibrated_duration.append(t_calib)\n",
    "                        nonconformal_score_2.append(np.inf)\n",
    "                        nonconformal_score_2 = np.array(nonconformal_score_2)\n",
    "                        np.savetxt('./data/nonconformal_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',nonconformal_score_2)\n",
    "\n",
    "                        np.savetxt('./data/calibrated_duration_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(calibrated_duration))\n",
    "                  \n",
    "                  sort_indices_2 = np.argsort(nonconformal_score_2)\n",
    "                  sorted_nonconformal_score_2 = nonconformal_score_2[sort_indices_2]\n",
    "                  \n",
    "                  # 再选择100个作为test point，以对应的抽样概率抽样\n",
    "                  included_1 = 0\n",
    "                  included_censor_1 = 0\n",
    "                  included_non_censor_1 = 0\n",
    "\n",
    "                  included_2 = 0\n",
    "                  included_censor_2 = 0\n",
    "                  included_non_censor_2 = 0\n",
    "                  \n",
    "                  \n",
    "                  sample_test_idx = rng.choice(test_idx,size=100,p=sampling_probs)\n",
    "                  num_non_censor = sum(event_test[sample_test_idx])\n",
    "                  interval_test_1 = []\n",
    "                  interval_test_2 = []\n",
    "                  for test_point_idx2 in sample_test_idx:\n",
    "                        sorted_weights = np.append(weights[sort_indices[:-1]],kernel_weights[test_point_idx2,test_point_idx])\n",
    "                        sorted_weights /= sorted_weights.sum()\n",
    "                        sorted_weight_cum_sum = np.cumsum(sorted_weights)\n",
    "                        CI_idx = np.min(np.argwhere(sorted_weight_cum_sum>=alpha))\n",
    "                        \n",
    "                        threshold_score = sorted_nonconformal_score_1[int(CI_idx)]\n",
    "                        \n",
    "                        x_test = X_test[test_point_idx2].reshape(1,-1)\n",
    "                        cumulative_hazards_test = wcp.model.predict_cumulative_hazards(x_test)\n",
    "                        exp_g_test = cumulative_hazards_test.loc[wcp.non_zero_idx]/wcp.bh\n",
    "                        exp_g_r_test = np.exp(np.log(exp_g_test) - threshold_score)\n",
    "                        candidate_idx = np.argwhere(exp_R_list<=exp_g_r_test[0])\n",
    "\n",
    "                        if len(candidate_idx) == 0:\n",
    "                              t_test_1 = max_duration\n",
    "                        else:\n",
    "                              t_test_1 = np.min(candidate_idx)*step + min_duration\n",
    "                        interval_test_1.append(t_test_1)\n",
    "                        if t_test_1 >= duration_test[test_point_idx2]:\n",
    "                              included_1 += 1\n",
    "                              if (event_test[test_point_idx2] == 0):\n",
    "                                    included_censor_1 += 1\n",
    "                              else:\n",
    "                                    included_non_censor_1 += 1\n",
    "                        \n",
    "                        # 计算测试点在第二个算法下的区间\n",
    "                        sorted_weights_2 = np.append(weights[sort_indices_2[:-1]],kernel_weights[test_point_idx2,test_point_idx])\n",
    "                        sorted_weights_2 /= sorted_weights_2.sum()\n",
    "                        sorted_weight_cum_sum_2 = np.cumsum(sorted_weights_2)\n",
    "                        CI_idx_2 = np.min(np.argwhere(sorted_weight_cum_sum_2>=alpha))\n",
    "                        \n",
    "                        threshold_score_2 = sorted_nonconformal_score_2[int(CI_idx_2)]\n",
    "                        if threshold_score_2 >= -t_test_1/2:\n",
    "                              t_test_2 = t_test_1 + threshold_score_2\n",
    "                        else:\n",
    "                              t_test_2 = -threshold_score_2\n",
    "\n",
    "                        interval_test_2.append(t_test_2)\n",
    "\n",
    "                        if t_test_2 >= duration_test[test_point_idx2]:\n",
    "                              included_2 += 1\n",
    "                              if (event_test[test_point_idx2] == 0):\n",
    "                                    included_censor_2 += 1\n",
    "                              else:\n",
    "                                    included_non_censor_2 += 1\n",
    "\n",
    "                  if num_non_censor == 0:\n",
    "                        non_censor_coverage.append(alpha)\n",
    "                        non_censor_coverage_2.append(alpha)\n",
    "                        censor_coverage.append(included_censor_1/(100-num_non_censor))\n",
    "                        censor_coverage_2.append(included_censor_2/(100-num_non_censor))\n",
    "                  elif num_non_censor == 100:\n",
    "                        censor_coverage.append(alpha)\n",
    "                        censor_coverage_2.append(alpha)\n",
    "                        non_censor_coverage.append(included_non_censor_1/num_non_censor)\n",
    "                        non_censor_coverage_2.append(included_non_censor_2/num_non_censor)\n",
    "                  else:\n",
    "                        censor_coverage.append(included_censor_1/(100-num_non_censor))\n",
    "                        censor_coverage_2.append(included_censor_2/(100-num_non_censor))\n",
    "                        non_censor_coverage.append(included_non_censor_1/num_non_censor)\n",
    "                        non_censor_coverage_2.append(included_non_censor_2/num_non_censor)\n",
    "                        \n",
    "                  coverage_CI.append(included_1/100)\n",
    "                  interval_len.append(np.mean(interval_test_1))\n",
    "                  coverage_CI_2.append(included_2/100)\n",
    "                  interval_test_2 = np.array(interval_test_2)\n",
    "                  interval_test_2 = interval_test_2[interval_test_2!=np.inf]\n",
    "                  interval_len_2.append(np.mean(interval_test_2))\n",
    "            t2 = datetime.now()\n",
    "            print('Time Elaspsed:\\t %.2f s'% ((t2-t1).total_seconds()))\n",
    "            print('[Epoch%d]\\t [Mean]\\t %.3f\\t[Std.]\\t%.3f'%(epoch,np.mean(coverage_CI),np.mean(coverage_CI_2)))\n",
    "            np.savetxt('./data/coverage_alg_1_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(coverage_CI))\n",
    "            empirical_coverage.append(np.mean(coverage_CI))\n",
    "            empirical_coverage_censor.append(np.mean(censor_coverage))\n",
    "            empirical_coverage_non_censor.append(np.mean(non_censor_coverage))\n",
    "\n",
    "            np.savetxt('./data/coverage_alg_2_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(coverage_CI_2))\n",
    "            empirical_coverage_2.append(np.mean(coverage_CI_2))\n",
    "            empirical_coverage_censor_2.append(np.mean(censor_coverage_2))\n",
    "            empirical_coverage_non_censor_2.append(np.mean(non_censor_coverage_2))\n",
    "            \n",
    "\n",
    "            interval_length_1.append(np.mean(interval_len))\n",
    "            interval_length_2.append(np.mean(interval_len_2))\n",
    "\n",
    "      print('Empirical Coverage of %d Epochs:\\t [Alg.1]\\t %.3f [Alg.2]\\t %.3f'%(epochs,np.mean(empirical_coverage),np.mean(empirical_coverage_2)))\n",
    "      print('Censor Data of %d Epochs:\\t [Alg.1]\\t %.3f [Alg.2]\\t %.3f'%(epochs,np.mean(empirical_coverage_censor),np.mean(empirical_coverage_censor_2)))\n",
    "      print('Non-Censor Data of %d Epochs:\\t [Alg.1]\\t %.3f [Alg.2]\\t %.3f'%(epochs,np.mean(empirical_coverage_non_censor),np.mean(empirical_coverage_non_censor_2)))\n",
    "\n",
    "      np.savetxt('./data/empirical_coverage_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage)\n",
    "      np.savetxt('./data/empirical_coverage_censor_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_censor)\n",
    "      np.savetxt('./data/empirical_coverage_non_censor_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_non_censor)\n",
    "      \n",
    "      np.savetxt('./data/empirical_coverage_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_2)\n",
    "      np.savetxt('./data/empirical_coverage_censor_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_censor_2)\n",
    "      np.savetxt('./data/empirical_coverage_non_censor_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_non_censor_2)\n",
    "\n",
    "      np.savetxt('./data/interval_length_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',interval_length_1)\n",
    "      np.savetxt('./data/interval_length_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',interval_length_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
