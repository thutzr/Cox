{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla CoxCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pycox\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.datasets import metabric\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import CoxCC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from generate_data import load_data\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from weighted_conformal_prediction_coxcc import WeightedConformalPrediction\n",
    "from datetime import datetime\n",
    "from pycox.datasets import metabric,support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "class WeightedConformalPrediction():\n",
    "    def __init__(self,df_train,train_frac = 0.8,num_nodes=[32,32],\n",
    "                 out_features=1,batch_norm=True,\n",
    "                 batch_size=128,dropout=0.1,output_bias=False,\n",
    "                epochs = 512, callbacks = [tt.callbacks.EarlyStopping()],\n",
    "                 verbose = True,classification_model='LR',\n",
    "                 percentile = 0.95,epsilon=0.01):\n",
    "        self.df_train = df_train\n",
    "        # self.p_t = len(self.df[self.df['event']==1])/len(self.df)\n",
    "        self.train_frac = train_frac\n",
    "        # self.cali_frac = calibration_frac\n",
    "        self.num_nodes = num_nodes\n",
    "        self.out_features = out_features\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.output_bias = output_bias\n",
    "        self.epochs = epochs\n",
    "        self.callbacks = callbacks\n",
    "        self.verbose = verbose\n",
    "        self.clf_model = classification_model\n",
    "        self.epsilon = epsilon\n",
    "        self.percentile = percentile\n",
    "        self.V = None\n",
    "        self.W = None\n",
    "        self.p_hat = None\n",
    "        self.T_h = None\n",
    "        self.x_mapper = None\n",
    "        self.get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "        self.bh = None\n",
    "        self.model = None\n",
    "        \n",
    "    # 划分数据，输入原始数据，选择划分的比例，输出训练集验证集和calibration set\n",
    "    def split_data(self):\n",
    "        random_idx = np.random.permutation(range(len(self.df_train)))\n",
    "        train_idx = random_idx[:int(len(self.df_train)*self.train_frac)]\n",
    "        val_idx = random_idx[int(len(self.df_train)*self.train_frac):]\n",
    "        self.Z_tr = self.df_train.iloc[train_idx,:]\n",
    "        self.Z_val = self.df_train.iloc[val_idx,:]\n",
    "    \n",
    "    def standardize(self):\n",
    "        cols_standardize = ['x0', 'x7', 'x8','x9','x10','x11','x12','x13']\n",
    "        cols_leave = ['x1', 'x2', 'x3', 'x4','x5','x6']\n",
    "        # cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "        # cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "#         cols_standardize = ['x0','x1','x2']\n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "        # polyfeature = [([col], PolynomialFeatures()) for col in cols_standardize]\n",
    "        self.x_mapper = DataFrameMapper(standardize+leave)\n",
    "        \n",
    "        self.x_train = self.x_mapper.fit_transform(self.Z_tr).astype('float32')\n",
    "        self.x_val = self.x_mapper.transform(self.Z_val).astype('float32')\n",
    "        # self.x_ca = self.x_mapper.transform(self.Z_ca).astype('float32')\n",
    "        \n",
    "        \n",
    "        self.y_train = self.get_target(self.Z_tr)\n",
    "        self.y_val = self.get_target(self.Z_val)\n",
    "        # self.durations_ca, self.events_ca = self.get_target(self.Z_ca)\n",
    "        self.val = self.x_val, self.y_val\n",
    "        self.in_features = self.x_train.shape[1]\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        self.split_data()\n",
    "        self.standardize()\n",
    "\n",
    "    def run_preprocessing(self):\n",
    "        if self.x_mapper == None:\n",
    "            self.preprocessing()\n",
    "        \n",
    "    def neural_network_cox(self):\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.in_features, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(0.1),\n",
    "\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(0.1),\n",
    "\n",
    "            torch.nn.Linear(32, self.out_features)\n",
    "        )\n",
    "        self.model = CoxCC(self.net, torch.optim.Adam)\n",
    "        self.model.fit(self.x_train, self.y_train, self.batch_size, self.epochs, self.callbacks, self.verbose,\n",
    "                val_data=self.val, val_batch_size=self.batch_size)\n",
    "        \n",
    "    def find_baseline_hazard_non_zero_idx(self):\n",
    "        if self.model == None:\n",
    "            self.neural_network_cox()\n",
    "        self.baseline_hazards = self.model.compute_baseline_hazards()\n",
    "        self.non_zero_idx = self.baseline_hazards[self.baseline_hazards>0].index[1] # 计算第一个非零元素的索引\n",
    "        self.bh = self.baseline_hazards.loc[self.non_zero_idx]\n",
    "        \n",
    "    def compute_nonconformal_score_single(self,t):\n",
    "        R = self.Z_tr[self.Z_tr['duration']>=t] # 找到at risk的人的covariates\n",
    "        if len(R) == 0: # 如果没找到at risk的人就跳过\n",
    "            return None\n",
    "        x_R = self.x_mapper.transform(R).astype('float32')\n",
    "        ch_r = self.model.predict_cumulative_hazards(x_R)\n",
    "        exp_g_r = ch_r.loc[self.non_zero_idx]/self.bh\n",
    "        return exp_g_r\n",
    "    \n",
    "    # 计算nonconformal score的函数，给定一个预测hazard的模型，training set\n",
    "    # 和calibration set以及base hazard，输出结果\n",
    "    def compute_nonconformal_score(self):\n",
    "        # print('WCP:compute nonconformal score')\n",
    "        if self.bh == None:\n",
    "            self.find_baseline_hazard_non_zero_idx()\n",
    "        Z_ca_1 = self.Z_ca[self.Z_ca['event']==1] # calibration set中发病的样本\n",
    "        x_ca = self.x_mapper.transform(Z_ca_1).astype('float32')\n",
    "        durations_test_1, events_test_1 = self.get_target(Z_ca_1)\n",
    "        cumulative_hazards = self.model.predict_cumulative_hazards(x_ca)\n",
    "        exp_g = cumulative_hazards.loc[self.non_zero_idx].div(self.bh)\n",
    "        self.V = list()\n",
    "        for i in range(len(x_ca)): # nonconformal score\n",
    "            exp_g_r = self.compute_nonconformal_score_single(durations_test_1[i])\n",
    "            if exp_g_r is None:\n",
    "                self.V.append(np.inf)\n",
    "            else:\n",
    "                self.V.append(np.log(exp_g[i])-np.log(np.sum(exp_g_r)+exp_g[i]))\n",
    "        print('[Mean]\\t%.2f\\t [Std.]\\t %.2f\\t[Max]\\t%.2f\\t[Min]\\t%.2f'%(np.mean(self.V),np.std(self.V),np.max(self.V),np.min(self.V)))\n",
    "        self.V = np.array(self.V+[np.inf])\n",
    "        \n",
    "    # 计算weight的函数，输入traning set, calibration set以及一个用来估计P(T=1|X=x)的分类模型\n",
    "    def compute_weight(self):\n",
    "        # print('WCP:compute weight')\n",
    "        Z_ca_1 = self.Z_ca[self.Z_ca['event']==1]\n",
    "        X_tr = self.x_train\n",
    "        X_ca = self.x_mapper.transform(Z_ca_1).astype('float32')\n",
    "        C_tr = self.Z_tr['event'] # training set的event,用于之后训练分类模型\n",
    "        # 根据输入选择分类模型\n",
    "        if self.clf_model == 'RF':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            self.clf = RandomForestClassifier(max_depth=6,random_state=0)\n",
    "        elif self.clf_model == 'LR':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            self.clf = LogisticRegression(random_state=0)\n",
    "        elif self.clf_model == 'XGBoost':\n",
    "            import xgboost as xgb\n",
    "            self.clf = xgb.XGBClassifier()\n",
    "        self.clf.fit(X_tr,C_tr) # 训练分类模型\n",
    "        p_predict = self.clf.predict_proba(X_ca)[:,1] # 预测p_hat\n",
    "        p_predict[p_predict<0.1] = 0.1\n",
    "        p_predict[p_predict>0.9] = 0.9\n",
    "        print(np.max(p_predict),np.min(p_predict))\n",
    "        self.W = np.divide(1-p_predict,p_predict) # 估计w_hat\n",
    "    \n",
    "    def run_compute_nonconformal_score(self):\n",
    "        if self.V == None:\n",
    "            self.compute_nonconformal_score()\n",
    "        else:\n",
    "            pass \n",
    "        \n",
    "    def run_conpute_weight(self):\n",
    "        if self.W == None:\n",
    "            self.compute_weight()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # 计算normalized weight,输入计算的weight，test point，训练过的分类模型\n",
    "    def compute_normalized_weight(self,x):\n",
    "        '''\n",
    "        x: test point\n",
    "        '''\n",
    "        # print('WCP:compute normalized weight')\n",
    "        p_predict = self.clf.predict_proba(x)[0,1] # 预测test point对应的T=1的概率\n",
    "        w_predict = self.p_t/p_predict # 估计p_hat\n",
    "        normalize_term = np.sum(self.W)+w_predict \n",
    "        p_hat = self.W/normalize_term # 计算所有病人的p_hat\n",
    "        p_inf = w_predict/normalize_term # 计算无穷点的weight\n",
    "\n",
    "        p_hat = np.append(p_hat,[p_inf])\n",
    "        return p_hat\n",
    " \n",
    "    # 计算对应的置信区间，输入nonconformal score, normalized weight p_hat, p_inf,以及指定的percentile\n",
    "    def compute_quantile(self,t,p_hat,exp_g_x):\n",
    "        exp_g_x_r = self.compute_nonconformal_score_single(t)\n",
    "        if exp_g_x_r is None:\n",
    "            return 1\n",
    "        V_x = np.log(exp_g_x)-np.log(np.sum(exp_g_x_r))\n",
    "        p_hat_leave = p_hat[self.V<=V_x[0]]\n",
    "        return sum(p_hat_leave)\n",
    "    \n",
    "    def weighted_conformal_prediction(self,x,percentile=0.95):\n",
    "        \n",
    "        ch = self.model.predict_cumulative_hazards(x)\n",
    "        exp_g_x = ch.loc[self.non_zero_idx]/self.bh\n",
    "\n",
    "        p_hat = self.compute_normalized_weight(x)\n",
    "\n",
    "        if percentile < 0.5:\n",
    "            quantile = 1\n",
    "            t = 5\n",
    "            quantile = self.compute_quantile(t,p_hat,exp_g_x)\n",
    "            while (quantile > percentile):\n",
    "                step = t*(quantile-percentile)\n",
    "                if step < 0.01:\n",
    "                    step = 0.01\n",
    "                t = t - sgn*step\n",
    "                if int(t) < 0:\n",
    "                    t = 0\n",
    "                    break\n",
    "                exp_g_x_r = self.compute_nonconformal_score_single(t)\n",
    "                V_x = np.log(exp_g_x)-np.log(np.sum(exp_g_x_r))\n",
    "                quantile = sum(p_hat[self.V<=V_x[0]])\n",
    "                print(quantile,t)\n",
    "\n",
    "            t_l = 0\n",
    "            t_h = t\n",
    "        else:\n",
    "            quantile_l,quantile_h = 1,0\n",
    "            t_l = 5\n",
    "            t_h = 5\n",
    "            quantile_l = self.compute_quantile(t_l,p_hat,exp_g_x)\n",
    "            while (quantile_l>(1-percentile)/2):\n",
    "                step_l = t_l*(quantile_l-(1-percentile)/2)\n",
    "                if step_l < 0.01:\n",
    "                    step_l = 0.01\n",
    "                t_l = t_l - step_l\n",
    "                if int(t_l) <= 0:\n",
    "                    t_l = 0\n",
    "                    break\n",
    "                exp_g_x_r = self.compute_nonconformal_score_single(t_l)\n",
    "                if exp_g_x_r is None:\n",
    "                    quantile_l = 1\n",
    "                    continue\n",
    "                V_x = np.log(exp_g_x)-np.log(np.sum(exp_g_x_r))\n",
    "                quantile_l = sum(p_hat[self.V<=V_x[0]])\n",
    "                # print(quantile_l,t_l)\n",
    "                \n",
    "            quantile_h = self.compute_quantile(t_h,p_hat,exp_g_x)\n",
    "            while (quantile_h<(0.5+self.percentile/2)):\n",
    "                step_h = t_h*(0.5+self.percentile/2-quantile_h)\n",
    "                if step_h < 0.01:\n",
    "                    step_h = 0.01\n",
    "                t_h = t_h + step_h\n",
    "                exp_g_x_r = self.compute_nonconformal_score_single(t_h)\n",
    "                if exp_g_x_r is None:\n",
    "                    quantile_h = 1\n",
    "                    continue\n",
    "                V_x = np.log(exp_g_x)-np.log(np.sum(exp_g_x_r))\n",
    "                # print(V_x)\n",
    "                quantile_h = sum(p_hat[self.V<=V_x[0]])\n",
    "            \n",
    "        return (t_l, t_h)\n",
    "\n",
    "    def run_training_step(self):\n",
    "        print('--'*30)\n",
    "        print('Begin Preprcessing Algorithm 1')\n",
    "        self.run_preprocessing()\n",
    "        print('--'*30)\n",
    "        try:\n",
    "            self.load_parameters(V_path = './model_data/V_alg_1.txt',W_path='./model_data/W_alg_1.txt',bh_path='./model_data/bh.txt',clf_path='./model_data/clf.model',model_path='./model_data/net_cox.model')\n",
    "            print('Loading Parameters From Files')\n",
    "        except:\n",
    "            print('Begin Noncoformal Score')\n",
    "            self.run_compute_nonconformal_score()\n",
    "            print('--'*30)\n",
    "            print('Begin Compute Wieght')\n",
    "            self.run_conpute_weight()\n",
    "            self.save_parmeters(V_path = './model_data/V_alg_1.txt',W_path='./model_data/W_alg_1.txt',bh_path='./model_data/bh.txt',clf_path='./model_data/clf.model',model_path='./model_data/net_cox.model')\n",
    "        plt.hist(self.W,bins=100)\n",
    "        plt.savefig('W.pdf')\n",
    "        print(len(self.W),len(self.V))\n",
    "            \n",
    "    def get_T(self,x,percentile=0.95):\n",
    "        t_l,t_h = self.weighted_conformal_prediction(x,percentile)\n",
    "        return (t_l,t_h)\n",
    "    \n",
    "    def get_nonconformal_score_of_calibration(self):\n",
    "        if self.V is None:\n",
    "            self.compute_nonconformal_score()\n",
    "        return self.V\n",
    "    \n",
    "    def get_weight(self):\n",
    "        if self.W is None:\n",
    "            self.compute_weight()\n",
    "        return self.W\n",
    "    \n",
    "    def get_normalized_weight(self,x):\n",
    "        if self.p_hat is None:\n",
    "            self.compute_normalized_weight(x)\n",
    "        \n",
    "        return self.p_hat\n",
    "\n",
    "    def save_parmeters(self,V_path = 'V_alg_1.txt',W_path='W_alg_1.txt',bh_path='bh.txt',clf_path='clf.model',model_path='net_cox.model'):\n",
    "        np.savetxt(V_path,self.V)\n",
    "        np.savetxt(W_path,self.W)\n",
    "        np.savetxt(bh_path,np.array([self.bh,self.non_zero_idx]))\n",
    "        joblib.dump(self.clf,clf_path)\n",
    "        joblib.dump(self.model,model_path)\n",
    "\n",
    "\n",
    "    def load_parameters(self,V_path = 'V_alg_1.txt',W_path='W_alg_1.txt',bh_path='bh.txt',clf_path='clf.model',model_path='net_cox.model'):\n",
    "        self.V = np.loadtxt(V_path)\n",
    "        self.W = np.loadtxt(W_path)\n",
    "        self.bh = float(np.loadtxt(bh_path)[0])\n",
    "        self.non_zero_idx = float(np.loadtxt(bh_path)[1])\n",
    "        self.clf = joblib.load(clf_path)\n",
    "        self.model = joblib.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\t0.700\t191.000\t1052.000\n",
      "[1]\t0.683\t179.000\t1033.000\n",
      "[2]\t0.690\t173.000\t1051.000\n",
      "[3]\t0.678\t176.000\t1027.000\n",
      "[4]\t0.700\t199.000\t1044.000\n",
      "[5]\t0.691\t183.000\t1043.000\n",
      "[6]\t0.697\t184.000\t1053.000\n",
      "[7]\t0.700\t191.000\t1051.000\n",
      "[8]\t0.689\t171.000\t1052.000\n",
      "[9]\t0.673\t185.000\t1010.000\n",
      "Total Coverage Statistics:\t [Mean]0.690\t[Std.]0.009\t[Max]0.700\t[Min]0.673\n",
      "[0]\t0.869\t395.000\t1147.000\n",
      "[1]\t0.852\t387.000\t1126.000\n",
      "[2]\t0.852\t371.000\t1141.000\n",
      "[3]\t0.856\t371.000\t1148.000\n",
      "[4]\t0.867\t370.000\t1169.000\n",
      "[5]\t0.856\t382.000\t1137.000\n",
      "[6]\t0.860\t386.000\t1140.000\n",
      "[7]\t0.872\t409.000\t1139.000\n",
      "[8]\t0.864\t376.000\t1157.000\n",
      "[9]\t0.854\t389.000\t1126.000\n",
      "Total Coverage Statistics:\t [Mean]0.860\t[Std.]0.007\t[Max]0.872\t[Min]0.852\n",
      "[0]\t0.988\t542.000\t1212.000\n",
      "[1]\t0.973\t542.000\t1185.000\n",
      "[2]\t0.963\t519.000\t1191.000\n",
      "[3]\t0.977\t525.000\t1210.000\n",
      "[4]\t0.988\t526.000\t1228.000\n",
      "[5]\t0.974\t531.000\t1197.000\n",
      "[6]\t0.976\t540.000\t1193.000\n",
      "[7]\t0.985\t559.000\t1190.000\n",
      "[8]\t0.975\t523.000\t1207.000\n",
      "[9]\t0.990\t566.000\t1191.000\n",
      "Total Coverage Statistics:\t [Mean]0.979\t[Std.]0.008\t[Max]0.990\t[Min]0.963\n",
      "[0]\t1.000\t558.000\t1217.000\n",
      "[1]\t1.000\t574.000\t1201.000\n",
      "[2]\t1.000\t576.000\t1199.000\n",
      "[3]\t1.000\t559.000\t1216.000\n",
      "[4]\t1.000\t541.000\t1234.000\n",
      "[5]\t1.000\t567.000\t1208.000\n",
      "[6]\t1.000\t575.000\t1200.000\n",
      "[7]\t1.000\t579.000\t1196.000\n",
      "[8]\t1.000\t558.000\t1217.000\n",
      "[9]\t1.000\t581.000\t1194.000\n",
      "Total Coverage Statistics:\t [Mean]1.000\t[Std.]0.000\t[Max]1.000\t[Min]1.000\n",
      "[0]\t1.000\t558.000\t1217.000\n",
      "[1]\t1.000\t574.000\t1201.000\n",
      "[2]\t1.000\t576.000\t1199.000\n",
      "[3]\t1.000\t559.000\t1216.000\n",
      "[4]\t1.000\t541.000\t1234.000\n",
      "[5]\t1.000\t567.000\t1208.000\n",
      "[6]\t1.000\t575.000\t1200.000\n",
      "[7]\t1.000\t579.000\t1196.000\n",
      "[8]\t1.000\t558.000\t1217.000\n",
      "[9]\t1.000\t581.000\t1194.000\n",
      "Total Coverage Statistics:\t [Mean]1.000\t[Std.]0.000\t[Max]1.000\t[Min]1.000\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.6,0.7,0.8,0.9,0.95]\n",
    "\n",
    "method = 'vanilla_cox_ph'\n",
    "data = 'support'\n",
    "\n",
    "df = support.read_df()\n",
    "epochs = 10\n",
    "train_frac = 0.6\n",
    "test_frac = 0.2\n",
    "val_frac = 0.2\n",
    "for alpha in alphas:\n",
    "    coverage = []\n",
    "    coverage_censor = []\n",
    "    coverage_non_censor = []\n",
    "    interval_len = []\n",
    "    for epoch in range(epochs):\n",
    "          rng = np.random.RandomState(epoch)\n",
    "          shuffle_idx = rng.permutation(range(len(df)))\n",
    "          train_idx = shuffle_idx[:int(train_frac*len(df))]\n",
    "          val_idx = shuffle_idx[int(train_frac*len(df)):int((train_frac+val_frac)*len(df))]\n",
    "          test_idx = shuffle_idx[int((train_frac+val_frac)*len(df)):]\n",
    "\n",
    "          df_train = df.iloc[train_idx,:]\n",
    "          df_val = df.iloc[val_idx,:]\n",
    "          df_test = df.iloc[test_idx,:]\n",
    "\n",
    "          cols_standardize = ['x0', 'x1', 'x2']\n",
    "\n",
    "          standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "          polyfeature = [([col], PolynomialFeatures()) for col in cols_standardize]\n",
    "          x_mapper = DataFrameMapper(standardize+polyfeature)\n",
    "\n",
    "          x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "          x_val = x_mapper.transform(df_val).astype('float32')\n",
    "          x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "          get_target = lambda df:(df['duration'].values,df['event'].values)\n",
    "          y_train = get_target(df_train)\n",
    "          y_val = get_target(df_val)\n",
    "          duration_test, event_test = get_target(df_test)\n",
    "\n",
    "          val = tt.tuplefy(x_val,y_val)\n",
    "\n",
    "          in_features = x_train.shape[1]\n",
    "          num_nodes = [32,32]\n",
    "          out_features = 1\n",
    "          batch_norm = True\n",
    "          dropout = 0.1\n",
    "          output_bias = False\n",
    "\n",
    "          net = tt.practical.MLPVanilla(in_features=in_features, num_nodes=num_nodes, out_features = out_features, batch_norm = batch_norm, dropout = dropout, output_bias = output_bias)\n",
    "\n",
    "          model = CoxCC(net,torch.optim.Adam)\n",
    "\n",
    "          batch_size = 256\n",
    "          n_epochs = 512\n",
    "          verbose = False\n",
    "          callbacks = [tt.callbacks.EarlyStopping()]\n",
    "          model.fit(x_train,y_train,batch_size,n_epochs,callbacks,verbose,val_data=val.repeat(10).cat())\n",
    "          _ = model.compute_baseline_hazards()\n",
    "\n",
    "          surv = model.predict_surv_df(x_test)\n",
    "          surv_ = (surv<=1-alpha).to_numpy(dtype='int8')\n",
    "          index = np.array(surv.index)\n",
    "          multiply_surv = np.transpose(surv_)*index\n",
    "          multiply_surv_ = np.where(multiply_surv==0,np.max(index),multiply_surv)\n",
    "\n",
    "          t_predict = multiply_surv_.min(axis = 1)\n",
    "          diff_predict_true = np.subtract(t_predict,np.array(df_test['duration']))\n",
    "\n",
    "          cover = sum(diff_predict_true>=0)/len(t_predict)\n",
    "          censor = 0\n",
    "          non_censor = 0\n",
    "          for i in range(len(df_test)):\n",
    "                if (diff_predict_true[i] >= 0):\n",
    "                      if (event_test[i]==0):\n",
    "                            censor += 1\n",
    "                      else:\n",
    "                            non_censor += 1\n",
    "\n",
    "          coverage.append(cover)\n",
    "          interval_len.append(np.mean(diff_predict_true))\n",
    "          n_censor = len(df_test) - sum(df_test['event'])\n",
    "          if n_censor == 0:\n",
    "                coverage_censor.append(alpha)\n",
    "                coverage_non_censor.append(cover)\n",
    "          elif n_censor == len(df_test):\n",
    "                coverage_non_censor.append(alpha)\n",
    "                coverage_censor.append(cover)\n",
    "          else:\n",
    "                coverage_censor.append(censor/n_censor)\n",
    "                coverage_non_censor.append(non_censor/(len(df_test)-n_censor))\n",
    "\n",
    "          print('[%d]\\t%.3f\\t%.3f\\t%.3f'%(epoch,cover,censor,non_censor))\n",
    "        \n",
    "    print('Total Coverage Statistics:\\t [Mean]%.3f\\t[Std.]%.3f\\t[Max]%.3f\\t[Min]%.3f'%(np.mean(coverage),np.std(coverage),np.max(coverage),np.min(coverage)))\n",
    "    np.savetxt('./output/vanilla_coxph_interval_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(interval_len))\n",
    "    np.savetxt('./output/vanilla_coxph_coverage_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(coverage))\n",
    "    np.savetxt('./output/vanilla_coxph_censor_coverage_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(coverage_censor))\n",
    "    np.savetxt('./output/vanilla_coxph_non_censor_coverage_'+data+'_'+str(alpha)+'_'+str(epochs)+'.txt',np.array(coverage_non_censor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoxCC+WCCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------|\n",
      "[Method]\tcox_cc\t[Dataset]\tsupport\t[Alpha]\t0.9\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 157.97 s\n",
      "[Epoch0]\t [Mean]\t 0.986\t[Std.]\t0.998\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 188.44 s\n",
      "[Epoch1]\t [Mean]\t 0.983\t[Std.]\t0.992\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 188.10 s\n",
      "[Epoch2]\t [Mean]\t 0.977\t[Std.]\t0.988\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 192.13 s\n",
      "[Epoch3]\t [Mean]\t 0.985\t[Std.]\t0.995\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 187.76 s\n",
      "[Epoch4]\t [Mean]\t 0.989\t[Std.]\t0.996\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 187.41 s\n",
      "[Epoch5]\t [Mean]\t 0.978\t[Std.]\t0.994\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 188.90 s\n",
      "[Epoch6]\t [Mean]\t 0.974\t[Std.]\t0.994\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 188.91 s\n",
      "[Epoch7]\t [Mean]\t 0.985\t[Std.]\t0.994\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 188.26 s\n",
      "[Epoch8]\t [Mean]\t 0.988\t[Std.]\t0.998\n",
      "------------------------------------------------------------|\n",
      "Initializing Algorithms\n",
      "------------------------------------------------------------|\n",
      "Pre-compute Hazard At Risk\n",
      "------------------------------------------------------------|\n",
      "Begin Computing Nonconformal Score of Alg. 1\n",
      "------------------------------------------------------------|\n",
      "Begin Testing\n",
      "Time Elaspsed:\t 188.27 s\n",
      "[Epoch9]\t [Mean]\t 0.976\t[Std.]\t0.989\n",
      "Empirical Coverage of 10 Epochs:\t [Alg.1]\t 0.982 [Alg.2]\t 0.994\n",
      "Censor Data of 10 Epochs:\t [Alg.1]\t 0.835 [Alg.2]\t 0.929\n",
      "Non-Censor Data of 10 Epochs:\t [Alg.1]\t 0.984 [Alg.2]\t 0.993\n"
     ]
    }
   ],
   "source": [
    "method = 'cox_cc'\n",
    "data = 'support'\n",
    "print('--'*30+'|')\n",
    "print('[Method]\\t'+method+'\\t[Dataset]\\t'+data+'\\t[Alpha]\\t'+str(alpha))\n",
    "df = support.read_df()\n",
    "max_duration = max(df['duration'])\n",
    "alphas = [0.95]\n",
    "for alpha in alphas:\n",
    "      \n",
    "      df_data = df\n",
    "      epochs = 10\n",
    "      train_frac = 0.8\n",
    "      empirical_coverage = []\n",
    "      empirical_coverage_censor = []\n",
    "      empirical_coverage_non_censor = []\n",
    "      empirical_coverage_2 = []\n",
    "      empirical_coverage_censor_2 = []\n",
    "      empirical_coverage_non_censor_2 = []\n",
    "      interval_length_1 = []\n",
    "      interval_length_2 = []\n",
    "      for epoch in range(epochs):\n",
    "            rng = np.random.RandomState(epoch)\n",
    "            shuffle_idx = rng.permutation(range(len(df_data)))\n",
    "            train_idx = shuffle_idx[:int(train_frac*len(df_data))]\n",
    "            calib_test_idx = shuffle_idx[int(train_frac*len(df_data)):]\n",
    "            df_train = df_data.iloc[train_idx,:]\n",
    "            df_calib_test = df_data.iloc[calib_test_idx,:]\n",
    "            df_calib_test.reset_index(drop = True,inplace = True)\n",
    "            print('--'*30+'|')\n",
    "            print('Initializing Algorithms')\n",
    "            wcp = WeightedConformalPrediction(df_train,verbose = False,percentile=alpha)\n",
    "            wcp.run_preprocessing()\n",
    "            wcp.find_baseline_hazard_non_zero_idx()\n",
    "            # 对所有可能的时间计算对用的at risk的人，以及相应的exp_g\n",
    "            print('--'*30+'|')\n",
    "            print('Pre-compute Hazard At Risk')\n",
    "            max_duration = np.max(df_data['duration'])\n",
    "            min_duration = np.min(df_data['duration'])\n",
    "            step = 0.1\n",
    "            num_steps = int((max_duration-min_duration)/step)+1\n",
    "\n",
    "            try:\n",
    "                  exp_R_list = np.loadtxt('./data/exp_R_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "            except:\n",
    "                  exp_R_list = []\n",
    "                  for i in range(num_steps):\n",
    "                        t = min_duration + i*step\n",
    "                        at_risk = df_train[df_train['duration']>=t]\n",
    "                        if len(at_risk) == 0:\n",
    "                              exp_R_list.append(0)\n",
    "                        else:\n",
    "                              x_risk = wcp.x_mapper.transform(at_risk).astype('float32')\n",
    "                              cumulative_hazards_risk = wcp.model.predict_cumulative_hazards(x_risk)\n",
    "                              exp_R_list.append(np.sum(cumulative_hazards_risk.loc[wcp.non_zero_idx]/wcp.bh))\n",
    "\n",
    "                  np.savetxt('./data/exp_R_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(exp_R_list))\n",
    "\n",
    "            X_test = wcp.x_mapper.transform(df_calib_test)\n",
    "            duration_test,event_test = wcp.get_target(df_calib_test)\n",
    "\n",
    "            sq_dists = squareform(pdist(X_test,'sqeuclidean'))\n",
    "            kernel_weights = np.exp(-sq_dists)\n",
    "\n",
    "            random_shuffle =rng.permutation(X_test.shape[0])\n",
    "            n_test_start_idx = X_test.shape[0] // 2\n",
    "            n_calib_2_start_idx = X_test.shape[0] // 4\n",
    "\n",
    "            calibration_indices = random_shuffle[:n_test_start_idx]\n",
    "            calib_2_idx = random_shuffle[n_test_start_idx:n_test_start_idx+n_calib_2_start_idx]\n",
    "            test_idx = random_shuffle[n_test_start_idx+n_calib_2_start_idx:]\n",
    "            # 计算 calibration set 的 non conformal score\n",
    "            print('--'*30+'|')\n",
    "            print('Begin Computing Nonconformal Score of Alg. 1')\n",
    "            df_calib = df_calib_test.iloc[calibration_indices,:]\n",
    "            df_calib_2 = df_calib_test.iloc[calib_2_idx,:]\n",
    "            df_test = df_calib_test.iloc[test_idx,:]\n",
    "\n",
    "\n",
    "            df_calib_non_censor = df_calib[df_calib['event']==1]\n",
    "            duration_test_non_censor,event_test_non_censor = wcp.get_target(df_calib_non_censor)\n",
    "            calib_non_censor_idx = np.array(df_calib_non_censor.index)\n",
    "            x_ca_non_censor = wcp.x_mapper.transform(df_calib_non_censor).astype('float32')\n",
    "            cumulative_hazards_non_censor = wcp.model.predict_cumulative_hazards(x_ca_non_censor)\n",
    "            exp_g_non_censor = cumulative_hazards_non_censor.loc[wcp.non_zero_idx]/wcp.bh\n",
    "\n",
    "            try:\n",
    "                  nonconformal_score_1 = np.loadtxt('./data/nonconformal_alg_1_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "            except:\n",
    "                  nonconformal_score_1 = []\n",
    "                  for i in range(len(df_calib_non_censor)):\n",
    "                        t = int((duration_test_non_censor[i]-min_duration)/step)\n",
    "                        exp_g_r = exp_R_list[t]\n",
    "                        nonconformal_score_1.append(np.log(exp_g_non_censor[i])-np.log(exp_g_r))\n",
    "\n",
    "                  nonconformal_score_1.append(np.inf)\n",
    "                  nonconformal_score_1 = np.array(nonconformal_score_1)\n",
    "                  np.savetxt('./data/nonconformal_alg_1_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',nonconformal_score_1)\n",
    "\n",
    "            sort_indices = np.argsort(nonconformal_score_1)\n",
    "            sorted_nonconformal_score_1 = nonconformal_score_1[sort_indices]\n",
    "            # 随机选取100个x_0\n",
    "            print('--'*30+'|')\n",
    "            print('Begin Testing')\n",
    "            coverage_CI = []\n",
    "            censor_coverage = []\n",
    "            non_censor_coverage = []\n",
    "            interval_len = []\n",
    "\n",
    "            coverage_CI_2 = []\n",
    "            censor_coverage_2 = []\n",
    "            non_censor_coverage_2 = []\n",
    "            interval_len_2 = []\n",
    "\n",
    "            t1 = datetime.now()\n",
    "            for test_point_idx in test_idx[:100]:\n",
    "\n",
    "                  weights = kernel_weights[test_point_idx][calib_non_censor_idx]\n",
    "                  sampling_probs = kernel_weights[test_point_idx][test_idx] # 计算测试集中每个点的抽样概率\n",
    "                  sampling_probs /= sampling_probs.sum()\n",
    "\n",
    "                  # 对第二个calibration set中的点计算nonconformalty score\n",
    "                  try:\n",
    "                        nonconformal_score_2 = np.loadtxt('./data/nonconformal_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "                        calibration_duration = np.loadtxt('./data/calibrated_duration_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt')\n",
    "                  except:\n",
    "                        nonconformal_score_2 = []\n",
    "                        calibrated_duration = []\n",
    "                        for calib_point_2 in calib_2_idx:\n",
    "                              sorted_calib_weights = np.append(weights[sort_indices[:-1]],kernel_weights[calib_point_2,test_point_idx])\n",
    "                              sorted_calib_weights /= sorted_calib_weights.sum()\n",
    "                              sorted_calib_weight_cum_sum = np.cumsum(sorted_calib_weights)\n",
    "                              CI_calib_idx = np.min(np.argwhere(sorted_calib_weight_cum_sum>=alpha))\n",
    "\n",
    "                              threshold_score_calib = sorted_nonconformal_score_1[int(CI_calib_idx)]\n",
    "\n",
    "                              x_test = X_test[calib_point_2].reshape(1,-1)\n",
    "                              cumulative_hazards_calib_2 = wcp.model.predict_cumulative_hazards(x_test)\n",
    "                              exp_g_calib_2 = cumulative_hazards_calib_2.loc[wcp.non_zero_idx]/wcp.bh\n",
    "                              exp_g_r_calib_2 = np.exp(np.log(exp_g_calib_2) - threshold_score_calib)\n",
    "                              candidate_calib_idx = np.argwhere(exp_R_list>=exp_g_r_calib_2[0])\n",
    "\n",
    "                              if len(candidate_calib_idx) == 0:\n",
    "                                    t_calib = max_duration\n",
    "                              else:\n",
    "                                    t_calib = np.max(candidate_calib_idx)*step + min_duration\n",
    "                              nonconformal_score_2.append(max(-duration_test[calib_point_2],duration_test[calib_point_2]-t_calib))\n",
    "\n",
    "                              calibrated_duration.append(t_calib)\n",
    "                        nonconformal_score_2.append(np.inf)\n",
    "                        nonconformal_score_2 = np.array(nonconformal_score_2)\n",
    "                        np.savetxt('./data/nonconformal_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',nonconformal_score_2)\n",
    "\n",
    "                        np.savetxt('./data/calibrated_duration_alg_2_'+str(test_point_idx)+'_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(calibrated_duration))\n",
    "\n",
    "                  sort_indices_2 = np.argsort(nonconformal_score_2)\n",
    "                  sorted_nonconformal_score_2 = nonconformal_score_2[sort_indices_2]\n",
    "\n",
    "                  # 再选择100个作为test point，以对应的抽样概率抽样\n",
    "                  included_1 = 0\n",
    "                  included_censor_1 = 0\n",
    "                  included_non_censor_1 = 0\n",
    "\n",
    "                  included_2 = 0\n",
    "                  included_censor_2 = 0\n",
    "                  included_non_censor_2 = 0\n",
    "\n",
    "\n",
    "                  sample_test_idx = rng.choice(test_idx,size=100,p=sampling_probs)\n",
    "                  num_non_censor = sum(event_test[sample_test_idx])\n",
    "                  interval_test_1 = []\n",
    "                  interval_test_2 = []\n",
    "                  for test_point_idx2 in sample_test_idx:\n",
    "                        sorted_weights = np.append(weights[sort_indices[:-1]],kernel_weights[test_point_idx2,test_point_idx])\n",
    "                        sorted_weights /= sorted_weights.sum()\n",
    "                        sorted_weight_cum_sum = np.cumsum(sorted_weights)\n",
    "                        CI_idx = np.min(np.argwhere(sorted_weight_cum_sum>=alpha))\n",
    "\n",
    "                        threshold_score = sorted_nonconformal_score_1[int(CI_idx)]\n",
    "\n",
    "                        x_test = X_test[test_point_idx2].reshape(1,-1)\n",
    "                        cumulative_hazards_test = wcp.model.predict_cumulative_hazards(x_test)\n",
    "                        exp_g_test = cumulative_hazards_test.loc[wcp.non_zero_idx]/wcp.bh\n",
    "                        exp_g_r_test = np.exp(np.log(exp_g_test) - threshold_score)\n",
    "                        candidate_idx = np.argwhere(exp_R_list<=exp_g_r_test[0])\n",
    "\n",
    "                        if len(candidate_idx) == 0:\n",
    "                              t_test_1 = max_duration\n",
    "                        else:\n",
    "                              t_test_1 = np.min(candidate_idx)*step + min_duration\n",
    "                        interval_test_1.append(t_test_1)\n",
    "                        if t_test_1 >= duration_test[test_point_idx2]:\n",
    "                              included_1 += 1\n",
    "                              if (event_test[test_point_idx2] == 0):\n",
    "                                    included_censor_1 += 1\n",
    "                              else:\n",
    "                                    included_non_censor_1 += 1\n",
    "\n",
    "                        # 计算测试点在第二个算法下的区间\n",
    "                        sorted_weights_2 = np.append(weights[sort_indices_2[:-1]],kernel_weights[test_point_idx2,test_point_idx])\n",
    "                        sorted_weights_2 /= sorted_weights_2.sum()\n",
    "                        sorted_weight_cum_sum_2 = np.cumsum(sorted_weights_2)\n",
    "                        CI_idx_2 = np.min(np.argwhere(sorted_weight_cum_sum_2>=alpha))\n",
    "\n",
    "                        threshold_score_2 = sorted_nonconformal_score_2[int(CI_idx_2)]\n",
    "                        if threshold_score_2 >= -t_test_1/2:\n",
    "                              t_test_2 = t_test_1 + threshold_score_2\n",
    "                        else:\n",
    "                              t_test_2 = -threshold_score_2\n",
    "                        if t_test_2 == np.inf:\n",
    "                            t_test_2 = max_duration\n",
    "#                         print(t_test_1,t_test_2,threshold_score_2)\n",
    "                        interval_test_2.append(t_test_2)\n",
    "\n",
    "                        if t_test_2 >= duration_test[test_point_idx2]:\n",
    "                              included_2 += 1\n",
    "                              if (event_test[test_point_idx2] == 0):\n",
    "                                    included_censor_2 += 1\n",
    "                              else:\n",
    "                                    included_non_censor_2 += 1\n",
    "\n",
    "                  if num_non_censor == 0:\n",
    "                        non_censor_coverage.append(alpha)\n",
    "                        non_censor_coverage_2.append(alpha)\n",
    "                        censor_coverage.append(included_censor_1/(100-num_non_censor))\n",
    "                        censor_coverage_2.append(included_censor_2/(100-num_non_censor))\n",
    "                  elif num_non_censor == 100:\n",
    "                        censor_coverage.append(alpha)\n",
    "                        censor_coverage_2.append(alpha)\n",
    "                        non_censor_coverage.append(included_non_censor_1/num_non_censor)\n",
    "                        non_censor_coverage_2.append(included_non_censor_2/num_non_censor)\n",
    "                  else:\n",
    "                        censor_coverage.append(included_censor_1/(100-num_non_censor))\n",
    "                        censor_coverage_2.append(included_censor_2/(100-num_non_censor))\n",
    "                        non_censor_coverage.append(included_non_censor_1/num_non_censor)\n",
    "                        non_censor_coverage_2.append(included_non_censor_2/num_non_censor)\n",
    "\n",
    "                  coverage_CI.append(included_1/100)\n",
    "                  interval_len.append(np.mean(interval_test_1))\n",
    "                  coverage_CI_2.append(included_2/100)\n",
    "                  interval_test_2 = np.array(interval_test_2)\n",
    "                  interval_test_2 = interval_test_2[interval_test_2!=np.inf]\n",
    "                  interval_len_2.append(np.mean(interval_test_2))\n",
    "            t2 = datetime.now()\n",
    "            print('Time Elaspsed:\\t %.2f s'% ((t2-t1).total_seconds()))\n",
    "            print('[Epoch%d]\\t [Mean]\\t %.3f\\t[Std.]\\t%.3f'%(epoch,np.mean(coverage_CI),np.mean(coverage_CI_2)))\n",
    "            np.savetxt('./data/coverage_alg_1_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(coverage_CI))\n",
    "            empirical_coverage.append(np.mean(coverage_CI))\n",
    "            empirical_coverage_censor.append(np.mean(censor_coverage))\n",
    "            empirical_coverage_non_censor.append(np.mean(non_censor_coverage))\n",
    "\n",
    "            np.savetxt('./data/coverage_alg_2_'+str(epoch)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',np.array(coverage_CI_2))\n",
    "            empirical_coverage_2.append(np.mean(coverage_CI_2))\n",
    "            empirical_coverage_censor_2.append(np.mean(censor_coverage_2))\n",
    "            empirical_coverage_non_censor_2.append(np.mean(non_censor_coverage_2))\n",
    "\n",
    "\n",
    "            interval_length_1.append(np.mean(interval_len))\n",
    "            interval_length_2.append(np.mean(interval_len_2))\n",
    "\n",
    "      print('Empirical Coverage of %d Epochs:\\t [Alg.1]\\t %.3f [Alg.2]\\t %.3f'%(epochs,np.mean(empirical_coverage),np.mean(empirical_coverage_2)))\n",
    "      print('Censor Data of %d Epochs:\\t [Alg.1]\\t %.3f [Alg.2]\\t %.3f'%(epochs,np.mean(empirical_coverage_censor),np.mean(empirical_coverage_censor_2)))\n",
    "      print('Non-Censor Data of %d Epochs:\\t [Alg.1]\\t %.3f [Alg.2]\\t %.3f'%(epochs,np.mean(empirical_coverage_non_censor),np.mean(empirical_coverage_non_censor_2)))\n",
    "\n",
    "      np.savetxt('./output/empirical_coverage_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage)\n",
    "      np.savetxt('./output/empirical_coverage_censor_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_censor)\n",
    "      np.savetxt('./output/empirical_coverage_non_censor_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_non_censor)\n",
    "\n",
    "      np.savetxt('./output/empirical_coverage_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_2)\n",
    "      np.savetxt('./output/empirical_coverage_censor_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_censor_2)\n",
    "      np.savetxt('./output/empirical_coverage_non_censor_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',empirical_coverage_non_censor_2)\n",
    "\n",
    "      np.savetxt('./output/interval_length_alg_1_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',interval_length_1)\n",
    "      np.savetxt('./output/interval_length_alg_2_'+str(epochs)+'_'+method+'_'+data+'_'+str(alpha)+'.txt',interval_length_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
