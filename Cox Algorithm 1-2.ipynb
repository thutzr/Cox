{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pycox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.datasets import metabric\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import CoxPH\n",
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     30
    ]
   },
   "outputs": [],
   "source": [
    "class WeightedConformalPrediction():\n",
    "    def __init__(self,df,train_frac=0.6,calibration_frac=0.2,num_nodes=[32,32],\n",
    "                 out_features=1,batch_norm=True,\n",
    "                 batch_size=256,dropout=0.1,output_bias=False,\n",
    "                epochs = 512, callbacks = [tt.callbacks.EarlyStopping()],\n",
    "                 verbose = True,classification_model='XGBoost',\n",
    "                 percentile = 0.95,epsilon=0.01):\n",
    "        self.df = df\n",
    "        self.p_t = len(self.df[self.df['event']==1])/len(self.df)\n",
    "        self.train_frac = train_frac\n",
    "        self.cali_frac = calibration_frac\n",
    "        self.num_nodes = num_nodes\n",
    "        self.out_features = out_features\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.output_bias = output_bias\n",
    "        self.epochs = epochs\n",
    "        self.callbacks = callbacks\n",
    "        self.verbose = verbose\n",
    "        self.clf_model = classification_model\n",
    "        self.epsilon = epsilon\n",
    "        self.percentile = percentile\n",
    "        self.V = None\n",
    "        self.W = None\n",
    "        self.p_hat = None\n",
    "        self.T_h = None\n",
    "        self.x_mapper = None\n",
    "        self.get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "        self.bh = None\n",
    "        self.model = None\n",
    "        \n",
    "    # 划分数据，输入原始数据，选择划分的比例，输出训练集验证集和calibration set\n",
    "    def split_data(self):\n",
    "        self.Z_tr = self.df.sample(frac=self.train_frac)\n",
    "        self.df = self.df.drop(self.Z_tr.index)\n",
    "        self.Z_ca = self.df.sample(frac=self.cali_frac/(1-self.train_frac))\n",
    "        self.df = self.df.drop(self.Z_ca.index)\n",
    "        self.Z_val = self.df\n",
    "    \n",
    "    def standardize(self):\n",
    "        cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "        cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "\n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "        self.x_mapper = DataFrameMapper(standardize + leave)\n",
    "        \n",
    "        self.x_train = self.x_mapper.fit_transform(self.Z_tr).astype('float32')\n",
    "        self.x_val = self.x_mapper.transform(self.Z_val).astype('float32')\n",
    "        self.x_ca = self.x_mapper.transform(self.Z_ca).astype('float32')\n",
    "        \n",
    "        \n",
    "        self.y_train = self.get_target(self.Z_tr)\n",
    "        self.y_val = self.get_target(self.Z_val)\n",
    "        self.durations_ca, self.events_ca = self.get_target(self.Z_ca)\n",
    "        self.val = self.x_val, self.y_val\n",
    "        self.in_features = self.x_train.shape[1]\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        self.split_data()\n",
    "        self.standardize()\n",
    "        \n",
    "    def neural_network_cox(self):\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.in_features, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(0.1),\n",
    "\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(0.1),\n",
    "\n",
    "            torch.nn.Linear(32, self.out_features)\n",
    "        )\n",
    "        self.model = CoxPH(self.net, torch.optim.Adam)\n",
    "        self.model.fit(self.x_train, self.y_train, self.batch_size, self.epochs, self.callbacks, self.verbose,\n",
    "                val_data=self.val, val_batch_size=self.batch_size)\n",
    "        \n",
    "    def find_baseline_hazard_non_zero_idx(self):\n",
    "        if self.model == None:\n",
    "            self.neural_network_cox()\n",
    "        self.baseline_hazards = self.model.compute_baseline_hazards()\n",
    "        self.non_zero_idx = self.baseline_hazards[self.baseline_hazards>0].index[1] # 计算第一个非零元素的索引\n",
    "        self.bh = self.baseline_hazards.loc[self.non_zero_idx]\n",
    "        \n",
    "    def compute_nonconformal_score_single(self,x,t):\n",
    "        R = self.Z_tr[self.Z_tr['duration']>=t] # 找到at risk的人的covariates\n",
    "        if len(R) == 0: # 如果没找到at risk的人就跳过\n",
    "            return None\n",
    "        x_R = self.x_mapper.transform(R).astype('float32')\n",
    "        ch_r = self.model.predict_cumulative_hazards(x_R)\n",
    "        exp_g_r = ch_r.loc[self.non_zero_idx]/self.bh\n",
    "        \n",
    "        return exp_g_r\n",
    "    \n",
    "    # 计算nonconformal score的函数，给定一个预测hazard的模型，training set\n",
    "    # 和calibration set以及base hazard，输出结果\n",
    "    def compute_nonconformal_score(self):\n",
    "        print('compute nonconformal score')\n",
    "        if self.x_mapper == None:\n",
    "            self.preprocessing()\n",
    "        if self.bh == None:\n",
    "            self.find_baseline_hazard_non_zero_idx()\n",
    "        Z_ca_1 = self.Z_ca[self.Z_ca['event']==1] # calibration set中发病的样本\n",
    "        x_ca = self.x_mapper.transform(Z_ca_1).astype('float32')\n",
    "        durations_test_1, events_test_1 = self.get_target(Z_ca_1)\n",
    "        cumulative_hazards = self.model.predict_cumulative_hazards(x_ca)\n",
    "        exp_g = cumulative_hazards.loc[self.non_zero_idx].div(self.bh)\n",
    "        self.V = list()\n",
    "        for i in range(len(x_ca)): # nonconformal score\n",
    "            exp_g_r = self.compute_nonconformal_score_single(x_ca[i],durations_test_1[i])\n",
    "            if exp_g_r is None:\n",
    "                self.V.append(np.inf)\n",
    "            else:\n",
    "                self.V.append(np.log(exp_g[i])-np.log(np.sum(exp_g_r)))\n",
    "        self.V = np.array(self.V+[np.inf])\n",
    "        \n",
    "    # 计算weight的函数，输入traning set, calibration set以及一个用来估计P(T=1|X=x)的分类模型\n",
    "    def compute_weight(self):\n",
    "        print('compute weight')\n",
    "        Z_ca_1 = self.Z_ca[self.Z_ca['event']==1]\n",
    "        X_tr = self.x_train\n",
    "        X_ca = self.x_mapper.transform(Z_ca_1).astype('float32')\n",
    "        C_tr = self.Z_tr.iloc[:,-1] # training set的event,用于之后训练分类模型\n",
    "        # 根据输入选择分类模型\n",
    "        if self.clf_model == 'RF':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            self.clf = RandomForestClassifier(max_depth=2,random_state=0)\n",
    "        elif self.clf_model == 'LR':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            self.clf = LogisticRegression(random_state=0)\n",
    "        elif self.clf_model == 'XGBoost':\n",
    "            import xgboost as xgb\n",
    "            self.clf = xgb.XGBClassifier()\n",
    "        self.clf.fit(X_tr,C_tr) # 训练分类模型\n",
    "        p_predict = self.clf.predict_proba(X_ca)[:,1] # 预测p_hat\n",
    "        self.W = np.divide(self.p_t,p_predict) # 估计w_hat\n",
    "        \n",
    "    # 计算normalized weight,输入计算的weight，test point，训练过的分类模型\n",
    "    def compute_normalized_weight(self,x):\n",
    "        '''\n",
    "        x: test point\n",
    "        '''\n",
    "        print('compute normalized weight')\n",
    "        if self.W == None:\n",
    "            self.compute_weight()\n",
    "        p_predict = self.clf.predict_proba(x)[0,1] # 预测test point对应的T=1的概率\n",
    "        w_predict = self.p_t/p_predict # 估计p_hat\n",
    "        normalize_term = np.sum(self.W)+w_predict \n",
    "        p_hat = [i/normalize_term for i in self.W] # 计算所有病人的p_hat\n",
    "        p_inf = w_predict/normalize_term # 计算无穷点的weight\n",
    "\n",
    "        self.p_hat = np.array(p_hat+[p_inf])\n",
    " \n",
    "    # 计算对应的置信区间，输入nonconformal score, normalized weight p_hat, p_inf,以及指定的percentile\n",
    "    def compute_quantile(self,x,t):\n",
    "        ch = self.model.predict_cumulative_hazards(x)\n",
    "        exp_g_x = ch.loc[self.non_zero_idx]/self.bh\n",
    "        exp_g_x_r = self.compute_nonconformal_score_single(x,t)\n",
    "        if exp_g_x_r is None:\n",
    "            return 1\n",
    "        V_x = np.log(exp_g_x)-np.log(np.sum(exp_g_x_r))\n",
    "        p_hat_leave = self.p_hat[self.V<=V_x[0]]\n",
    "        \n",
    "        return sum(p_hat_leave)\n",
    "    \n",
    "    def weighted_conformal_prediction(self,x):\n",
    "        if self.V == None:\n",
    "            self.compute_nonconformal_score()\n",
    "        self.compute_normalized_weight(x)\n",
    "        print('weighted conformal prediction')\n",
    "        quantile = 0\n",
    "        t_l = 0\n",
    "        t_h = 100\n",
    "        while (quantile<self.percentile):\n",
    "            t_h = t_h*(1/(self.percentile-quantile))\n",
    "            quantile = self.compute_quantile(x,t_h)\n",
    "        quantile_ = quantile\n",
    "        t_l = t_h-10\n",
    "        while (quantile-self.percentile>self.epsilon):\n",
    "            quantile = self.compute_quantile(x,t_l)\n",
    "            \n",
    "            if quantile < self.percentile:\n",
    "                self.T_h = t_l+10\n",
    "                break\n",
    "            t_l = t_l - 10\n",
    "        self.T_h = t_l\n",
    "                \n",
    "#         while (quantile<self.percentile):\n",
    "#             t = (t_l+t_h)/2\n",
    "#             quantile = self.compute_quantile(x,t)\n",
    "#             print(quantile,t,t_l,t_h)\n",
    "#             if (quantile >= self.percentile) and (abs(quantile-self.percentile)>self.epsilon):\n",
    "#                 t_h = t\n",
    "#                 t = (t_h+t_l)/2\n",
    "#             else:\n",
    "#                 t_l = t\n",
    "#                 t = (t_h+t_l)/2\n",
    "#             self.T_h = t\n",
    "            \n",
    "    def get_T(self,x,t_h):\n",
    "        if self.T_h is None:\n",
    "            self.weighted_conformal_prediction(x)\n",
    "        return self.T_h\n",
    "    \n",
    "    def get_nonconformal_score_of_calibration(self):\n",
    "        if self.V is None:\n",
    "            self.compute_nonconformal_score()\n",
    "        return self.V\n",
    "    \n",
    "    def get_weight(self):\n",
    "        if self.W is None:\n",
    "            self.compute_weight()\n",
    "        return self.W\n",
    "    \n",
    "    def get_normalized_weight(self,x):\n",
    "        if self.p_hat is None:\n",
    "            self.compute_normalized_weight(x)\n",
    "        return self.p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute nonconformal score\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 4.9349,\tval_loss: 4.4688\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 4.7703,\tval_loss: 4.4529\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 4.6512,\tval_loss: 4.4320\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 4.6480,\tval_loss: 4.4125\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 4.6010,\tval_loss: 4.4008\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 4.5825,\tval_loss: 4.3956\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 4.5567,\tval_loss: 4.3962\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 4.5724,\tval_loss: 4.4010\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 4.5904,\tval_loss: 4.4040\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 4.5713,\tval_loss: 4.4020\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 4.5739,\tval_loss: 4.3984\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 4.5518,\tval_loss: 4.3959\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 4.5459,\tval_loss: 4.3967\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 4.5631,\tval_loss: 4.3964\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 4.5520,\tval_loss: 4.3953\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 4.5310,\tval_loss: 4.3949\n",
      "16:\t[0s / 1s],\t\ttrain_loss: 4.5671,\tval_loss: 4.3932\n",
      "17:\t[0s / 1s],\t\ttrain_loss: 4.5202,\tval_loss: 4.3856\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 4.5173,\tval_loss: 4.3810\n",
      "19:\t[0s / 1s],\t\ttrain_loss: 4.5180,\tval_loss: 4.3816\n",
      "20:\t[0s / 1s],\t\ttrain_loss: 4.5076,\tval_loss: 4.3853\n",
      "21:\t[0s / 1s],\t\ttrain_loss: 4.5488,\tval_loss: 4.3878\n",
      "22:\t[0s / 1s],\t\ttrain_loss: 4.5226,\tval_loss: 4.3886\n",
      "23:\t[0s / 1s],\t\ttrain_loss: 4.4959,\tval_loss: 4.3837\n",
      "24:\t[0s / 1s],\t\ttrain_loss: 4.5087,\tval_loss: 4.3810\n",
      "25:\t[0s / 1s],\t\ttrain_loss: 4.4990,\tval_loss: 4.3801\n",
      "26:\t[0s / 1s],\t\ttrain_loss: 4.4997,\tval_loss: 4.3799\n",
      "27:\t[0s / 1s],\t\ttrain_loss: 4.5171,\tval_loss: 4.3785\n",
      "28:\t[0s / 1s],\t\ttrain_loss: 4.5130,\tval_loss: 4.3771\n",
      "29:\t[0s / 1s],\t\ttrain_loss: 4.5090,\tval_loss: 4.3797\n",
      "30:\t[0s / 1s],\t\ttrain_loss: 4.4854,\tval_loss: 4.3796\n",
      "31:\t[0s / 1s],\t\ttrain_loss: 4.4968,\tval_loss: 4.3807\n",
      "32:\t[0s / 1s],\t\ttrain_loss: 4.5005,\tval_loss: 4.3818\n",
      "33:\t[0s / 1s],\t\ttrain_loss: 4.4744,\tval_loss: 4.3858\n",
      "34:\t[0s / 1s],\t\ttrain_loss: 4.5099,\tval_loss: 4.3844\n",
      "35:\t[0s / 1s],\t\ttrain_loss: 4.4724,\tval_loss: 4.3852\n",
      "36:\t[0s / 1s],\t\ttrain_loss: 4.5120,\tval_loss: 4.3838\n",
      "37:\t[0s / 1s],\t\ttrain_loss: 4.4753,\tval_loss: 4.3812\n",
      "38:\t[0s / 1s],\t\ttrain_loss: 4.4896,\tval_loss: 4.3814\n",
      "compute normalized weight\n",
      "compute weight\n",
      "weighted conformal prediction\n",
      "223.4213490723746\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_data = metabric.read_df()\n",
    "    wcp = WeightedConformalPrediction(df_data)\n",
    "    t_h = np.max(df_data['duration'])\n",
    "    x = np.array([[-0.91468555,1.1301656,-0.47822767,-0.789998,0.7613344,1.,1.,0. ,1.]]).astype('float32')\n",
    "    print(wcp.get_T(x,t_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = wcp.x_mapper.transform(df_data.iloc[10:11,:]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6580326 , -1.0016024 ,  0.507556  , -0.31917572,  0.2517567 ,\n",
       "         1.        ,  1.        ,  1.        ,  0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
